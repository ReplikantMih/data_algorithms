{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sG8r2RPbshw"
   },
   "source": [
    "Реализация деревьев решений была дважды продемонстрирована в предыдущих уроках, в этом не будем ее повторять и возьмем готовую реализацию дерева решений для регрессии из библиотеки `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qk-74OFhbshx"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn import model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYD1kLOibsh0"
   },
   "source": [
    "Используем один из \"игрушечных\" датасетов из той же библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDZbSvqMbsh1"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leu8bBI7bsh6"
   },
   "outputs": [],
   "source": [
    "# X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cz_JhiIpbsh8"
   },
   "source": [
    "Разделим выборку на обучающую и тестовую в соотношении 75/25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExZPR9FLbsh9"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xJCdggZbsh_"
   },
   "source": [
    "Напишем функцию, реализующую предсказание в градиентном бустинге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wU_Rkc63bsiA"
   },
   "outputs": [],
   "source": [
    "def gb_predict(X, trees_list, coef_list, eta):\n",
    "    # Реализуемый алгоритм градиентного бустинга будет инициализироваться нулевыми значениями,\n",
    "    # поэтому все деревья из списка trees_list уже являются дополнительными и при предсказании прибавляются с шагом eta\n",
    "    return np.array([sum([eta* coef * alg.predict([x])[0] for alg, coef in zip(trees_list, coef_list)]) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gZvsyfSbsiD"
   },
   "source": [
    "В качестве функционала ошибки будем использовать среднеквадратичную ошибку. Реализуем соответствующую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xbjFIEKbsiE"
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_real, prediction):\n",
    "    return (sum((y_real - prediction)**2)) / len(y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qlr3KD6zbsiH"
   },
   "source": [
    "Используем $L_{2}$ loss $L(y, z) = (y-z)^{2},$ ее производная по $z$ примет вид $L'(y, z) = 2(z-y)$. Реализуем ее также в виде функции (коэффициент 2 можно отбросить)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRaZEd3ebsiI"
   },
   "outputs": [],
   "source": [
    "def bias(y, z):\n",
    "    return (y - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ut-7dBgVbsiK"
   },
   "source": [
    "Реализуем функцию обучения градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoIdAoPYbsiL"
   },
   "outputs": [],
   "source": [
    "def gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta):\n",
    "    \n",
    "    # Деревья будем записывать в список\n",
    "    trees = []\n",
    "    \n",
    "    # Будем записывать ошибки на обучающей и тестовой выборке на каждой итерации в список\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "\n",
    "        # инициализируем бустинг начальным алгоритмом, возвращающим ноль, \n",
    "        # поэтому первый алгоритм просто обучаем на выборке и добавляем в список\n",
    "        if len(trees) == 0:\n",
    "            # обучаем первое дерево на обучающей выборке\n",
    "            tree.fit(X_train, y_train)\n",
    "            \n",
    "            train_errors.append(mean_squared_error(y_train, gb_predict(X_train, trees, coefs, eta)))\n",
    "            test_errors.append(mean_squared_error(y_test, gb_predict(X_test, trees, coefs, eta)))\n",
    "        else:\n",
    "            # Получим ответы на текущей композиции\n",
    "            target = gb_predict(X_train, trees, coefs, eta)\n",
    "            \n",
    "            # алгоритмы начиная со второго обучаем на сдвиг\n",
    "            tree.fit(X_train, bias(y_train, target))\n",
    "            \n",
    "            train_errors.append(mean_squared_error(y_train, gb_predict(X_train, trees, coefs, eta)))\n",
    "            test_errors.append(mean_squared_error(y_test, gb_predict(X_test, trees, coefs, eta)))\n",
    "\n",
    "        trees.append(tree)\n",
    "        \n",
    "    return trees, train_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDMFn3R-bsiR"
   },
   "outputs": [],
   "source": [
    "def evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta):\n",
    "    train_prediction = gb_predict(X_train, trees, coefs, eta)\n",
    "\n",
    "#     print(f'Ошибка алгоритма из {n_trees} деревьев глубиной {max_depth} \\\n",
    "#     с шагом {eta} на тренировочной выборке: {mean_squared_error(y_train, train_prediction)}')\n",
    "\n",
    "    test_prediction = gb_predict(X_test, trees, coefs, eta)\n",
    "\n",
    "#     print(f'Ошибка алгоритма из {n_trees} деревьев глубиной {max_depth} \\\n",
    "#     с шагом {eta} на тестовой выборке: {mean_squared_error(y_test, test_prediction)}')\n",
    "    \n",
    "    return(mean_squared_error(y_train, train_prediction), mean_squared_error(y_test, test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXrSdSgjbsic"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjDdKkgObsif"
   },
   "outputs": [],
   "source": [
    "def get_error_plot(n_trees, train_err, test_err):\n",
    "    plt.xlabel('Iteration number')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlim(0, n_trees)\n",
    "    plt.plot(list(range(n_trees)), train_err, label='train error')\n",
    "    plt.plot(list(range(n_trees)), test_err, label='test error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашняя работа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(max_depth, n_trees):\n",
    "    coefs = [1] * n_trees\n",
    "    trees, train_errors, test_errors = gb_fit(n_trees, max_depth, X_train, X_test, y_train, y_test, coefs, eta)\n",
    "    train_error, test_error = evaluate_alg(X_train, X_test, y_train, y_test, trees, coefs, eta)\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19647655786511664 0.21917951509191322\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "n_trees = 10\n",
    "\n",
    "max_depth_max = 25\n",
    "max_depth = 10\n",
    "\n",
    "errors = []\n",
    "train_errors, test_errors = process(max_depth=max_depth, n_trees=n_trees)\n",
    "errors.append((max_depth, train_errors, test_errors))\n",
    "print(train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### После понижения резмерности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eE6X-RyIdfJ-"
   },
   "outputs": [],
   "source": [
    "# Для начала отмасштабируем выборку\n",
    "X_ = X.astype(float)\n",
    "\n",
    "rows, cols = X_.shape\n",
    "\n",
    "# центрирование - вычитание из каждого значения среднего по строке\n",
    "means = X_.mean(0)\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        X_[i, j] -= means[j]\n",
    "\n",
    "# деление каждого значения на стандартное отклонение\n",
    "std = np.std(X_, axis=0)\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        X_[j][i] /= std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ItFOmLW9dfKB",
    "outputId": "0b507805-c106-4539-d15a-4dc8327b764f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собственные значения в порядке убывания:\n",
      "(437.7746724797988, array([ 0.52106591, -0.26934744,  0.5804131 ,  0.56485654]))\n",
      "(137.10457072021055, array([-0.37741762, -0.92329566, -0.02449161, -0.06694199]))\n",
      "(22.013531335697195, array([-0.71956635,  0.24438178,  0.14212637,  0.63427274]))\n",
      "(3.107225464292886, array([ 0.26128628, -0.12350962, -0.80144925,  0.52359713]))\n"
     ]
    }
   ],
   "source": [
    "# Найдем собственные векторы и собственные значения (англ. Eigenvalues)\n",
    " \n",
    "covariance_matrix = X_.T.dot(X_)\n",
    "\n",
    "eig_values, eig_vectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# сформируем список кортежей (собственное значение, собственный вектор)\n",
    "eig_pairs = [(np.abs(eig_values[i]), eig_vectors[:, i]) for i in range(len(eig_values))]\n",
    "\n",
    "# и отсортируем список по убыванию собственных значений\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('Собственные значения в порядке убывания:')\n",
    "for i in eig_pairs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mdEDwm2dfKD"
   },
   "source": [
    "Оценим долю дисперсии, которая описывается найденными компонентами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ct-1I70mdfKE",
    "outputId": "ab04c5c2-649d-4f6f-e849-3791203481d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля дисперсии, описываемая каждой из компонент \n",
      "[72.96244541329987, 22.850761786701778, 3.6689218892828697, 0.5178709107154814]\n",
      "Кумулятивная доля дисперсии по компонентам \n",
      "[ 72.96244541  95.8132072   99.48212909 100.        ]\n"
     ]
    }
   ],
   "source": [
    "eig_sum = sum(eig_values)\n",
    "var_exp = [(i / eig_sum) * 100 for i in sorted(eig_values, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Доля дисперсии, описываемая каждой из компонент \\n{var_exp}')\n",
    "\n",
    "# а теперя оценим кумулятивную (то есть накапливаемую) дисперсию при учитывании каждой из компонент\n",
    "print(f'Кумулятивная доля дисперсии по компонентам \\n{cum_var_exp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NE15sfLtdfKH"
   },
   "source": [
    "Таким образом, первая главная компонента описывает почти 73% информации, а первые две в сумме - 95.8%. В то же время последняя компонента описывает всего 0.5% и может быть отброжена без страха значительных потерь в качестве нашего анализа. Мы отбросим последние две компоненты, оставив первые две."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4xcGvf7dfKI",
    "outputId": "e406100f-3bd4-40fb-ce09-15db12e77a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица весов W:\n",
      " [[ 0.52106591 -0.37741762]\n",
      " [-0.26934744 -0.92329566]\n",
      " [ 0.5804131  -0.02449161]\n",
      " [ 0.56485654 -0.06694199]]\n"
     ]
    }
   ],
   "source": [
    "# Сформируем вектор весов из собственных векторов, соответствующих первым двум главным компонентам\n",
    "W = np.hstack((eig_pairs[0][1].reshape(X.shape[1],1), eig_pairs[1][1].reshape(X.shape[1],1)))\n",
    "\n",
    "print(f'Матрица весов W:\\n', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n04tyd6xdfKL"
   },
   "outputs": [],
   "source": [
    "# Сформируем новую матрицу \"объекты-признаки\"\n",
    "Z = X_.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Z, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19756206370967527 0.25114493723278714\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "train_errors_after, test_errors_after = process(max_depth=max_depth, n_trees=n_trees)\n",
    "errors.append((max_depth, train_errors, test_errors))\n",
    "print(train_errors_after, test_errors_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Влияние PCA на результаты работы модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка на train изменилась на 0.6 %\n",
      "Ошибка на test изменилась на 14.6 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Ошибка на train изменилась на {round(100 * (train_errors_after - train_errors) / train_errors, 1)} %')\n",
    "print(f'Ошибка на test изменилась на {round(100 * (test_errors_after - test_errors) / test_errors, 1)} %')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Rg5ilDbTbshs",
    "Z2tMUhhXbsjK",
    "gKn5SRC2bsjr",
    "qBVboqCmbsjs"
   ],
   "name": "Lesson_6(edited).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
